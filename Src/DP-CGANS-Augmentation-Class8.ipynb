{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNmuy4l83iaLRs7O9hJSRn0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nQyCIpya3-eo","executionInfo":{"status":"ok","timestamp":1705539582899,"user_tz":-360,"elapsed":223484,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"e154670d-4496-4ee6-fa9d-6cb847446371"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (2.0.1)\n","Collecting adversarial-robustness-toolbox\n","  Downloading adversarial_robustness_toolbox-1.17.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.23.5)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.11.4)\n","Collecting scikit-learn<1.2.0,>=0.22.2 (from adversarial-robustness-toolbox)\n","  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.1)\n","Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (3.2.0)\n","Installing collected packages: scikit-learn, adversarial-robustness-toolbox\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.2.2\n","    Uninstalling scikit-learn-1.2.2:\n","      Successfully uninstalled scikit-learn-1.2.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.19.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed adversarial-robustness-toolbox-1.17.0 scikit-learn-1.1.3\n","Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.10.1)\n","Collecting imbalanced-learn\n","  Downloading imbalanced_learn-0.11.0-py3-none-any.whl (235 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.6/235.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.23.5)\n","Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.11.4)\n","Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.1.3)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.2.0)\n","Installing collected packages: imbalanced-learn\n","  Attempting uninstall: imbalanced-learn\n","    Found existing installation: imbalanced-learn 0.10.1\n","    Uninstalling imbalanced-learn-0.10.1:\n","      Successfully uninstalled imbalanced-learn-0.10.1\n","Successfully installed imbalanced-learn-0.11.0\n","Collecting smartnoise-synth\n","  Downloading smartnoise_synth-1.0.3-py3-none-any.whl (83 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Faker<16.0.0,>=15.0.0 (from smartnoise-synth)\n","  Downloading Faker-15.3.4-py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting opacus<0.15.0,>=0.14.0 (from smartnoise-synth)\n","  Downloading opacus-0.14.0-py3-none-any.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.3/114.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pac-synth<0.0.9,>=0.0.8 (from smartnoise-synth)\n","  Downloading pac_synth-0.0.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting smartnoise-sql<2.0.0,>=1.0.3 (from smartnoise-synth)\n","  Downloading smartnoise_sql-1.0.3-py3-none-any.whl (144 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting torch<2.0.0 (from smartnoise-synth)\n","  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from Faker<16.0.0,>=15.0.0->smartnoise-synth) (2.8.2)\n","Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus<0.15.0,>=0.14.0->smartnoise-synth) (1.23.5)\n","Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus<0.15.0,>=0.14.0->smartnoise-synth) (1.11.4)\n","Requirement already satisfied: PyYAML<7.0.0,>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth) (6.0.1)\n","Collecting antlr4-python3-runtime==4.9.3 (from smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth)\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting graphviz<0.18,>=0.17 (from smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth)\n","  Downloading graphviz-0.17-py3-none-any.whl (18 kB)\n","Collecting opendp<0.9.0,>=0.8.0 (from smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth)\n","  Downloading opendp-0.8.0-py3-none-any.whl (30.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.1/30.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pandas<3.0.0,>=2.0.1 (from smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth)\n","  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sqlalchemy<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth) (2.0.24)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.0.0->smartnoise-synth) (4.5.0)\n","Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch<2.0.0->smartnoise-synth)\n","  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96 (from torch<2.0.0->smartnoise-synth)\n","  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66 (from torch<2.0.0->smartnoise-synth)\n","  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch<2.0.0->smartnoise-synth)\n","  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0->smartnoise-synth) (67.7.2)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2.0.0->smartnoise-synth) (0.42.0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0.0,>=2.0.1->smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth) (2023.3.post1)\n","Collecting tzdata>=2022.1 (from pandas<3.0.0,>=2.0.1->smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth)\n","  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->Faker<16.0.0,>=15.0.0->smartnoise-synth) (1.16.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3.0.0,>=2.0.0->smartnoise-sql<2.0.0,>=1.0.3->smartnoise-synth) (3.0.3)\n","Building wheels for collected packages: antlr4-python3-runtime\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=4a7da06d547b5fa6cbeba243f63c9af824137dab4026eb4d75f2549b3c255d1c\n","  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n","Successfully built antlr4-python3-runtime\n","Installing collected packages: antlr4-python3-runtime, tzdata, pac-synth, opendp, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, graphviz, pandas, nvidia-cudnn-cu11, Faker, torch, smartnoise-sql, opacus, smartnoise-synth\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.1\n","    Uninstalling graphviz-0.20.1:\n","      Successfully uninstalled graphviz-0.20.1\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","  Attempting uninstall: torch\n","    Found existing installation: torch 2.1.0+cu121\n","    Uninstalling torch-2.1.0+cu121:\n","      Successfully uninstalled torch-2.1.0+cu121\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","lida 0.0.10 requires fastapi, which is not installed.\n","lida 0.0.10 requires kaleido, which is not installed.\n","lida 0.0.10 requires python-multipart, which is not installed.\n","lida 0.0.10 requires uvicorn, which is not installed.\n","bigframes 0.19.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.1.4 which is incompatible.\n","bigframes 0.19.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.4 which is incompatible.\n","torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\n","torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed Faker-15.3.4 antlr4-python3-runtime-4.9.3 graphviz-0.17 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 opacus-0.14.0 opendp-0.8.0 pac-synth-0.0.8 pandas-2.1.4 smartnoise-sql-1.0.3 smartnoise-synth-1.0.3 torch-1.13.1 tzdata-2023.4\n"]}],"source":["!pip install xlrd\n","!pip install adversarial-robustness-toolbox\n","!pip install -U imbalanced-learn\n","!pip install smartnoise-synth"]},{"cell_type":"code","source":["import sys\n","import os\n","import pandas as pd\n","import numpy as np\n","import requests, io\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","from numpy import mean\n","from imblearn.pipeline import Pipeline\n","from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE, SVMSMOTE, KMeansSMOTE\n","from imblearn.under_sampling import RandomUnderSampler\n","from imblearn.combine import SMOTEENN, SMOTETomek\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from keras.layers import Dense\n","from keras.models import Sequential\n","from keras.utils import to_categorical\n","from keras.layers import Dropout\n","from keras.constraints import MaxNorm\n","from sklearn.neighbors import NearestNeighbors\n","from sklearn.datasets import make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import roc_curve\n","from sklearn.metrics import roc_auc_score\n","from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n","from art.estimators.classification import KerasClassifier\n","from sklearn.metrics import classification_report\n","from snsynth import Synthesizer\n","\n","tf.compat.v1.disable_eager_execution()\n","\n","drive.mount('/content/drive', force_remount=True)\n","DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/'\n","\n","dataset = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/UNSW_NB15_MIA.csv\")\n","dataset = dataset.dropna()\n","\n","dataset = dataset.drop(['sport', 'dsport', 'stcpb', 'dtcpb', 'Stime', 'Ltime', 'sbytes', 'dbytes', 'Sload', 'Dload'], axis=1)\n","dataset['proto'] = pd.Categorical(dataset['proto']).codes\n","dataset['state'] = pd.Categorical(dataset['state']).codes\n","dataset['service'] = pd.Categorical(dataset['service']).codes\n","dataset['attack_cat'] = pd.Categorical(dataset['attack_cat']).codes\n","ATTACK_CAT_MAP = {1:0,2:1,3:2,4:3, 5:4,6:5,7:6,8:7}\n","dataset['attack_cat'] = dataset['attack_cat'].replace(ATTACK_CAT_MAP)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hWNPhS6L4pgB","executionInfo":{"status":"ok","timestamp":1705539655115,"user_tz":-360,"elapsed":62582,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"38175c5e-2ca9-42e5-97a2-28816a60cfb2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["sm_dataset_0 = dataset.query(\"attack_cat == 0\").iloc[1:200]\n","sm_dataset_1 = dataset.query(\"attack_cat == 1\").iloc[1:200]\n","sm_dataset_2 = dataset.query(\"attack_cat == 2\").iloc[1:200]\n","sm_dataset_3 = dataset.query(\"attack_cat == 3\").iloc[1:200]\n","sm_dataset_4 = dataset.query(\"attack_cat == 4\").iloc[1:200]\n","sm_dataset_5 = dataset.query(\"attack_cat == 5\").iloc[1:200]\n","sm_dataset_6 = dataset.query(\"attack_cat == 6\").iloc[1:200]\n","sm_dataset_7 = dataset.query(\"attack_cat == 7\").iloc[1:200]\n","sm_dataset = pd.concat([sm_dataset_0, sm_dataset_1, sm_dataset_2, sm_dataset_3, sm_dataset_4,sm_dataset_5,sm_dataset_6,sm_dataset_7])\n","print(len(sm_dataset_0))\n","print(len(sm_dataset_1))\n","print(len(sm_dataset_2))\n","print(len(sm_dataset_3))\n","print(len(sm_dataset_4))\n","print(len(sm_dataset_5))\n","print(len(sm_dataset_6))\n","print(len(sm_dataset_7))\n","synthetic_data = pd.DataFrame()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5jwbUw2W5VI2","executionInfo":{"status":"ok","timestamp":1705539659630,"user_tz":-360,"elapsed":625,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"ae9a61c9-73d7-49c2-b097-8e750e51844c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["199\n","7\n","199\n","199\n","199\n","199\n","16\n","199\n"]}]},{"cell_type":"code","source":["for i in range(30):\n","  synth = Synthesizer.create('dpgan', epsilon=102.0)\n","  sample = synth.fit_sample(sm_dataset, preprocessor_eps=100.0)\n","  frames=[synthetic_data,sample]\n","  synthetic_data=pd.concat(frames)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1djKt9w75gdZ","executionInfo":{"status":"ok","timestamp":1705540053697,"user_tz":-360,"elapsed":383243,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"44e9f8f0-9229-4fb4-ac5a-04e4d90ae7e6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]},{"output_type":"stream","name":"stdout","text":["Spent 100.00000000000003 epsilon on preprocessor, leaving 1.9999999999999716 for training\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:638: UserWarning: The sample rate will be defined from ``batch_size`` and ``sample_size``.The returned privacy budget will be incorrect.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:229: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_rng`` turned on.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1117: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n","  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"]}]},{"cell_type":"code","source":["synthetic_data_0 = synthetic_data.query(\"attack_cat == 0\")\n","synthetic_data_1 = synthetic_data.query(\"attack_cat == 1\")\n","synthetic_data_2 = synthetic_data.query(\"attack_cat == 2\")\n","synthetic_data_3 = synthetic_data.query(\"attack_cat == 3\")\n","synthetic_data_4 = synthetic_data.query(\"attack_cat == 4\")\n","synthetic_data_5 = synthetic_data.query(\"attack_cat == 5\")\n","synthetic_data_6 = synthetic_data.query(\"attack_cat == 6\")\n","synthetic_data_7 = synthetic_data.query(\"attack_cat == 7\")\n","print(len(synthetic_data_0))\n","print(len(synthetic_data_1))\n","print(len(synthetic_data_2))\n","print(len(synthetic_data_3))\n","print(len(synthetic_data_5))\n","print(len(synthetic_data_6))\n","print(len(synthetic_data_7))\n","print(len(synthetic_data_7))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yafDMVpqn3IY","executionInfo":{"status":"ok","timestamp":1705540067134,"user_tz":-360,"elapsed":573,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"1a933311-a056-4a3e-cb49-d673c1612227"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["4910\n","3392\n","6934\n","4055\n","5814\n","2523\n","3812\n","3812\n"]}]},{"cell_type":"code","source":["sm_dataset_0 = dataset.query(\"attack_cat == 0\").iloc[1:200]\n","sm_dataset_1 = dataset.query(\"attack_cat == 1\").iloc[1:200]\n","sm_dataset_2 = dataset.query(\"attack_cat == 2\").iloc[1:200]\n","sm_dataset_3 = dataset.query(\"attack_cat == 3\").iloc[1:200]\n","sm_dataset_4 = dataset.query(\"attack_cat == 4\").iloc[1:200]\n","sm_dataset_5 = dataset.query(\"attack_cat == 5\").iloc[1:200]\n","sm_dataset_6 = dataset.query(\"attack_cat == 6\").iloc[1:200]\n","sm_dataset_7 = dataset.query(\"attack_cat == 7\").iloc[1:200]\n","sm_dataset = pd.concat([sm_dataset_0, sm_dataset_1, sm_dataset_2, sm_dataset_3, sm_dataset_4,sm_dataset_5,sm_dataset_6,sm_dataset_7])\n","frames = [sm_dataset, synthetic_data]\n","\n","augmented_data = pd.concat(frames)"],"metadata":{"id":"3825r-N3u11f","executionInfo":{"status":"ok","timestamp":1705540069972,"user_tz":-360,"elapsed":578,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["X = augmented_data.to_numpy()[:,:33]\n","y = augmented_data.to_numpy()[:,-1]\n","#X.shape\n","#y.shape\n","np.unique(y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScfSjafAvNDm","executionInfo":{"status":"ok","timestamp":1705540072999,"user_tz":-360,"elapsed":596,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"cfbf62d2-16ab-4d9b-d5cb-0f84a2e774c4"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 1., 2., 3., 4., 5., 6., 7.])"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["def _splitData(X,y):\n","  X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=104,test_size=0.5,shuffle=True)\n","  scaler = MinMaxScaler()\n","  label_encoder = LabelEncoder()\n","  X_train = scaler.fit_transform(X_train)\n","  X_test = scaler.fit_transform(X_test)\n","  label_encoder.fit(y_train)\n","  label_encoder.fit(y_test)\n","  y_train = label_encoder.transform(y_train)\n","  y_test = label_encoder.transform(y_test)\n","  return X_train, X_test, y_train, y_test\n","\n","def _buildModel(X_train, X_test, y_train, y_test):\n","  #os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","  optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.001)\n","  loss = tf.keras.losses.SparseCategoricalCrossentropy()\n","\n","  model = Sequential()\n","  model.add(Dense(33, activation='relu', input_dim=33, kernel_constraint=MaxNorm(3)))\n","  model.add(Dropout(0.3))\n","  model.add(Dense(64, activation='relu'))\n","  model.add(Dropout(0.3))\n","  model.add(Dense(32, activation='relu'))\n","  model.add(Dropout(0.3))\n","  model.add(Dense(16, activation='relu'))\n","  model.add(Dropout(0.3))\n","  model.add(Dense(8, activation='softmax'))\n","  model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n","  #model.summary()\n","  history = model.fit(X_train, y_train,\n","                        validation_data=(X_test, y_test),\n","                        batch_size=128,\n","                        epochs=30,\n","                        verbose=0)\n","  pred_val = model.predict(X_test)\n","  #fpr, tpr, threshold = roc_curve(y_test, pred_val, pos_label=1)\n","  #random_pred_val = [0 for i in range(len(y_test))]\n","  #p_fpr1, p_tpr1, _ = roc_curve(y_test, random_pred_val, pos_label=1)\n","  auc_score = roc_auc_score(y_test, pred_val, multi_class='ovr')\n","  print(f\"AUC SCORE: {auc_score:6f}\")\n","  return model, auc_score\n","\n","def calc_precision_recall(predicted, actual, positive_value=1):\n","    score = 0  # both predicted and actual are positive\n","    num_positive_predicted = 0  # predicted positive\n","    num_positive_actual = 0  # actual positive\n","    for i in range(len(predicted)):\n","        if predicted[i] == positive_value:\n","            num_positive_predicted += 1\n","        if actual[i] == positive_value:\n","            num_positive_actual += 1\n","        if predicted[i] == actual[i]:\n","            if predicted[i] == positive_value:\n","                score += 1\n","\n","    if num_positive_predicted == 0:\n","        precision = 1\n","    else:\n","        precision = score / num_positive_predicted  # the fraction of predicted “Yes” responses that are correct\n","    if num_positive_actual == 0:\n","        recall = 1\n","    else:\n","        recall = score / num_positive_actual  # the fraction of “Yes” responses that are predicted correctly\n","    return precision, recall\n","\n","def _attackMIA(X_train, X_test, y_train, y_test, model):\n","  attack_train_ratio = 0.5\n","  attack_train_size = int(len(X_train) * attack_train_ratio)\n","  attack_test_size = int(len(X_test) * attack_train_ratio)\n","\n","  mlp_art_model = KerasClassifier(model=model, clip_values=(0, 1))\n","\n","  mlp_attack_bb = MembershipInferenceBlackBox(mlp_art_model, attack_model_type='rf')\n","\n","  mlp_attack_bb.fit(X_train[:attack_train_size].astype(np.float32),y_train[:attack_train_size].astype(np.float32),\n","                    X_test[:attack_test_size].astype(np.float32), y_test[:attack_test_size].astype(np.float32))\n","  mlp_inferred_train_bb = mlp_attack_bb.infer(X_train[attack_train_size:].astype(np.float32), y_train[attack_train_size:])\n","  mlp_inferred_test_bb = mlp_attack_bb.infer(X_test[attack_test_size:].astype(np.float32), y_test[attack_test_size:])\n","\n","  mlp_train_acc_bb = np.sum(mlp_inferred_train_bb) / len(mlp_inferred_train_bb)\n","  mlp_test_acc_bb = 1-(np.sum(mlp_inferred_test_bb) / len(mlp_inferred_test_bb))\n","  mlp_acc_bb = (mlp_train_acc_bb * len(mlp_inferred_train_bb) + mlp_test_acc_bb * len(mlp_inferred_test_bb)) / (len(mlp_inferred_train_bb) + len(mlp_inferred_test_bb))\n","\n","  #print(f\"Members Accuracy: {mlp_train_acc_bb:.4f}\")\n","  #print(f\"Non Members Accuracy {mlp_test_acc_bb:.4f}\")\n","  #print(f\"Attack Accuracy {mlp_acc_bb:.4f}\")\n","\n","  precision, recall = calc_precision_recall(np.concatenate((mlp_inferred_train_bb, mlp_inferred_test_bb)),\n","                              np.concatenate((np.ones(len(mlp_inferred_train_bb)), np.zeros(len(mlp_inferred_test_bb)))))\n","  y_train_pred = np.concatenate((mlp_inferred_train_bb, mlp_inferred_test_bb))\n","  y_train_true = np.concatenate((np.ones_like(mlp_inferred_train_bb), np.zeros_like(mlp_inferred_test_bb)))\n","  #print(classification_report(y_pred=y_train_pred, y_true=y_train_true))\n","  return mlp_train_acc_bb, mlp_test_acc_bb, mlp_acc_bb, precision, recall\n"],"metadata":{"id":"Yrbif_KzviTx","executionInfo":{"status":"ok","timestamp":1705540074813,"user_tz":-360,"elapsed":11,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import concurrent.futures\n","import threading\n","\n","num_iterations = 20\n","auc = []\n","member  = []\n","nonmember = []\n","att = []\n","prec = []\n","rec = []\n","\n","def execAttack(lock):\n","  with lock:\n","    #print(\"Attack executed.\")\n","    X_train, X_test, y_train, y_test = _splitData(X,y)\n","    model, auc_score = _buildModel(X_train, X_test, y_train, y_test)\n","    mlp_train_acc_bb, mlp_test_acc_bb, mlp_acc_bb, precision, recall = _attackMIA(X_train, X_test, y_train, y_test, model)\n","    #print(f\"AUC SCORE,Members Accuracy,Non Members Accuracy,Attack Accuracy,Attack Precision,Attack Recall\")\n","    print(f\"{auc_score:4f},{mlp_train_acc_bb:4f},{mlp_test_acc_bb:4f},{mlp_acc_bb:4f},{precision:4f},{recall:4f}\")\n","    auc.append(auc_score)\n","    member.append(mlp_train_acc_bb)\n","    nonmember.append(mlp_test_acc_bb)\n","    att.append(mlp_acc_bb)\n","    prec.append(precision)\n","    rec.append(recall)\n","\n","with concurrent.futures.ThreadPoolExecutor() as executor:\n","    lock = threading.Lock()\n","    futures = [executor.submit(execAttack,lock) for _ in range(num_iterations)]\n","    concurrent.futures.wait(futures)\n","ppldf = pd.DataFrame()\n","ppldf['AUC'] = np.array(auc)\n","ppldf['Member_Accuracy'] = np.array(member)\n","ppldf['Non_Member_Accuracy'] = np.array(nonmember)\n","ppldf['Attack_accuracy'] = np.array(att)\n","ppldf['Precision'] = np.array(prec)\n","ppldf['Recall'] = np.array(rec)\n","#ppldf.head()\n","ppldf.to_csv(\"/content/drive/MyDrive/Colab Notebooks/results/DPGAN/Class-8/result_DPGAN_100_0.csv\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQmece8evkLn","executionInfo":{"status":"ok","timestamp":1705540666782,"user_tz":-360,"elapsed":580498,"user":{"displayName":"Subhasish Ghosh","userId":"09231805416479081409"}},"outputId":"21861878-54b7-4ba8-d52a-6ea0cae8aa51"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.740867\n","0.740867,0.708651,0.724449,0.716550,0.720026,0.708651\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.739666\n","0.739666,0.707379,0.694975,0.701177,0.698712,0.707379\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.730831\n","0.730831,0.760178,0.742366,0.751272,0.746875,0.760178\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.727033\n","0.727033,0.826336,0.816158,0.821247,0.818010,0.826336\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.725698\n","0.725698,0.777142,0.712574,0.744858,0.730007,0.777142\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.732307\n","0.732307,0.784351,0.771204,0.777778,0.774173,0.784351\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.696257\n","0.696257,0.799406,0.772371,0.785888,0.778363,0.799406\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.736683\n","0.736683,0.769402,0.754877,0.762140,0.758386,0.769402\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.711929\n","0.711929,0.785411,0.710242,0.747827,0.730500,0.785411\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.720061\n","0.720061,0.764843,0.754771,0.759807,0.757216,0.764843\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.737512\n","0.737512,0.764843,0.740246,0.752545,0.746482,0.764843\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.708898\n","0.708898,0.786684,0.781383,0.784033,0.782535,0.786684\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.717830\n","0.717830,0.800254,0.754877,0.777566,0.765517,0.800254\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.715531\n","0.715531,0.801209,0.753817,0.777513,0.764956,0.801209\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.714548\n","0.714548,0.751802,0.763889,0.757846,0.761000,0.751802\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.736593\n","0.736593,0.759224,0.761344,0.760284,0.760837,0.759224\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.712832\n","0.712832,0.812977,0.800254,0.806616,0.802764,0.812977\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.703295\n","0.703295,0.843511,0.812341,0.827926,0.818014,0.843511\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.687131\n","0.687131,0.831107,0.807464,0.819285,0.811911,0.831107\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2335: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates = self.state_updates\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n","  updates=self.state_updates,\n"]},{"output_type":"stream","name":"stdout","text":["AUC SCORE: 0.722118\n","0.722118,0.760390,0.701866,0.731128,0.718349,0.760390\n"]}]}]}